{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHRRuwVFQPfB7Hb1hNsKTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PPancham/PhD/blob/main/XGBoost_Model_and_LightGBM_Model_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxgLZ6rXCKRF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Function to save the model\n",
        "def SaveModel(model, prefix=\"Trained_Model_\"):\n",
        "    filename = f\"{prefix}{datetime.date.today()}.pkl\"\n",
        "    joblib.dump(model, filename)\n",
        "    return f\"Model saved to '{filename}'\"\n",
        "\n",
        "uploadedSpreadsheet = files.upload()\n",
        "fileName = list(uploadedSpreadsheet.keys())[0]\n",
        "data = pd.read_excel(fileName)\n",
        "\n",
        "# Define columns to process\n",
        "columns_to_process = ['IgG1 Average', 'IgG2 Average', 'IgG3 Average', 'IgG4 Average',\n",
        "                      'IgA Average', 'IgE Average', 'IgM Average']\n",
        "\n",
        "# Extract features and target\n",
        "features = data[columns_to_process]\n",
        "target = data['Group']\n",
        "\n",
        "# Apply label encoding to convert string class labels to integers\n",
        "print(\"\\nEncoding target variable...\")\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_target = label_encoder.fit_transform(target)\n",
        "print(f\"Original classes: {label_encoder.classes_}\")\n",
        "print(f\"Encoded as: {np.unique(encoded_target)}\")\n",
        "\n",
        "# Data preprocessing\n",
        "print(\"Data Preprocessing:\")\n",
        "\n",
        "# 1. Check for missing values\n",
        "print(\"\\nChecking for missing values...\")\n",
        "missing_values = features.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "# 2. Impute missing values if any\n",
        "if missing_values.sum() > 0:\n",
        "    print(\"Imputing missing values...\")\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    features = pd.DataFrame(imputer.fit_transform(features),\n",
        "                           columns=features.columns,\n",
        "                           index=features.index)\n",
        "\n",
        "# 3. Check for outliers using IQR method\n",
        "print(\"\\nChecking for outliers using IQR method...\")\n",
        "def detect_outliers_iqr(df):\n",
        "    outlier_indices = []\n",
        "    outlier_values = {}\n",
        "\n",
        "    for column in df.columns:\n",
        "        # Calculate Q1 and Q3\n",
        "        Q1 = df[column].quantile(0.25)\n",
        "        Q3 = df[column].quantile(0.75)\n",
        "\n",
        "        # Calculate IQR\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Define outlier bounds\n",
        "        outlier_lower = Q1 - 1.5 * IQR\n",
        "        outlier_upper = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Find outliers\n",
        "        column_outliers = df[(df[column] < outlier_lower) | (df[column] > outlier_upper)].index.tolist()\n",
        "        outlier_values[column] = df.loc[column_outliers, column].tolist()\n",
        "        outlier_indices.extend(column_outliers)\n",
        "\n",
        "    # Return unique indices of rows with outliers\n",
        "    return list(set(outlier_indices)), outlier_values\n",
        "\n",
        "outlier_indices, outlier_values = detect_outliers_iqr(features)\n",
        "print(f\"Found {len(outlier_indices)} rows with outliers\")\n",
        "\n",
        "# Optionally handle outliers - in this case, we'll keep them but log their presence\n",
        "for column, values in outlier_values.items():\n",
        "    if values:\n",
        "        print(f\"Column {column} has {len(values)} outliers\")\n",
        "\n",
        "# 4. Standardize the features for better model performance\n",
        "print(\"\\nStandardizing features...\")\n",
        "scaler = StandardScaler()\n",
        "scaled_features = pd.DataFrame(scaler.fit_transform(features),\n",
        "                              columns=features.columns,\n",
        "                              index=features.index)\n",
        "\n",
        "# Now we have preprocessed data ready for modeling\n",
        "print(\"\\nData preprocessing complete.\")\n",
        "\n",
        "# Split the data for initial evaluation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    scaled_features, encoded_target, test_size=0.3, random_state=42, stratify=encoded_target\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "# Model Training and Evaluation Functions\n",
        "def train_and_evaluate_xgboost(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate XGBoost model\"\"\"\n",
        "    print(\"\\n--- XGBoost Model Training and Evaluation ---\")\n",
        "\n",
        "    # Initialize and train basic XGBoost model\n",
        "    xgb_model = xgb.XGBClassifier(\n",
        "        objective='multi:softprob',\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        random_state=42,\n",
        "        verbosity=0\n",
        "    )\n",
        "\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = xgb_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Basic XGBoost Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(xgb_model, scaled_features, encoded_target, cv=5)\n",
        "    print(f\"Cross-validation scores: {cv_scores}\")\n",
        "    print(f\"Mean CV accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "    # Hyperparameter tuning with GridSearchCV\n",
        "    print(\"\\nPerforming hyperparameter tuning for XGBoost...\")\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'subsample': [0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "        'gamma': [0, 0.1, 0.2],\n",
        "        'min_child_weight': [1, 3, 5]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        xgb.XGBClassifier(objective='multi:softprob', use_label_encoder=False, eval_metric='mlogloss'),\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(scaled_features, encoded_target)\n",
        "\n",
        "    # Get best parameters and model\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nBest parameters: {best_params}\")\n",
        "    print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Evaluate best model on test set\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "    best_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "    print(f\"Best XGBoost Model Test Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(\"\\nClassification Report (Best XGBoost Model):\")\n",
        "    print(classification_report(y_test, y_pred_best))\n",
        "\n",
        "    # Print report with original class names for better readability\n",
        "    print(\"\\nClassification Report with original class names:\")\n",
        "    y_test_original = label_encoder.inverse_transform(y_test)\n",
        "    y_pred_original = label_encoder.inverse_transform(y_pred_best)\n",
        "    print(classification_report(y_test_original, y_pred_original))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix (Best XGBoost Model):\")\n",
        "    print(confusion_matrix(y_test, y_pred_best))\n",
        "\n",
        "    # Print confusion matrix with class labels\n",
        "    print(\"\\nConfusion Matrix with class labels:\")\n",
        "    conf_matrix = confusion_matrix(y_test_original, y_pred_original)\n",
        "    conf_df = pd.DataFrame(conf_matrix,\n",
        "                         index=label_encoder.classes_,\n",
        "                         columns=label_encoder.classes_)\n",
        "    print(conf_df)\n",
        "\n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': columns_to_process,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nFeature Importance (Best XGBoost Model):\")\n",
        "    print(feature_importance)\n",
        "\n",
        "    # Save the best model\n",
        "    save_result = SaveModel(best_model, \"XGBoost_Best_Model_\")\n",
        "    print(save_result)\n",
        "\n",
        "    return best_model, best_accuracy, feature_importance\n",
        "\n",
        "def train_and_evaluate_lightgbm(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate LightGBM model\"\"\"\n",
        "    print(\"\\n--- LightGBM Model Training and Evaluation ---\")\n",
        "\n",
        "    # Initialize and train basic LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        objective='multiclass',\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    )\n",
        "\n",
        "    lgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = lgb_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Basic LightGBM Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Cross-validation\n",
        "    cv_scores = cross_val_score(lgb_model, scaled_features, encoded_target, cv=5)\n",
        "    print(f\"Cross-validation scores: {cv_scores}\")\n",
        "    print(f\"Mean CV accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "    # Hyperparameter tuning with GridSearchCV\n",
        "    print(\"\\nPerforming hyperparameter tuning for LightGBM...\")\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'num_leaves': [31, 50, 70],\n",
        "        'subsample': [0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "        'min_child_samples': [5, 10, 20],\n",
        "        'reg_alpha': [0, 0.1, 0.5],\n",
        "        'reg_lambda': [0, 0.1, 0.5]\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        lgb.LGBMClassifier(objective='multiclass'),\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(scaled_features, encoded_target)\n",
        "\n",
        "    # Get best parameters and model\n",
        "    best_params = grid_search.best_params_\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nBest parameters: {best_params}\")\n",
        "    print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Evaluate best model on test set\n",
        "    y_pred_best = best_model.predict(X_test)\n",
        "    best_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "    print(f\"Best LightGBM Model Test Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(\"\\nClassification Report (Best LightGBM Model):\")\n",
        "    print(classification_report(y_test, y_pred_best))\n",
        "\n",
        "    # Print report with original class names for better readability\n",
        "    print(\"\\nClassification Report with original class names:\")\n",
        "    y_test_original = label_encoder.inverse_transform(y_test)\n",
        "    y_pred_original = label_encoder.inverse_transform(y_pred_best)\n",
        "    print(classification_report(y_test_original, y_pred_original))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    print(\"\\nConfusion Matrix (Best LightGBM Model):\")\n",
        "    print(confusion_matrix(y_test, y_pred_best))\n",
        "\n",
        "    # Print confusion matrix with class labels\n",
        "    print(\"\\nConfusion Matrix with class labels:\")\n",
        "    conf_matrix = confusion_matrix(y_test_original, y_pred_original)\n",
        "    conf_df = pd.DataFrame(conf_matrix,\n",
        "                         index=label_encoder.classes_,\n",
        "                         columns=label_encoder.classes_)\n",
        "    print(conf_df)\n",
        "\n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': columns_to_process,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nFeature Importance (Best LightGBM Model):\")\n",
        "    print(feature_importance)\n",
        "\n",
        "    # Save the best model\n",
        "    save_result = SaveModel(best_model, \"LightGBM_Best_Model_\")\n",
        "    print(save_result)\n",
        "\n",
        "    return best_model, best_accuracy, feature_importance\n",
        "\n",
        "# Execute the model training and evaluation\n",
        "print(\"\\nStarting model training and evaluation...\\n\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Train and evaluate XGBoost\n",
        "xgb_best_model, xgb_accuracy, xgb_feature_importance = train_and_evaluate_xgboost(X_train, X_test, y_train, y_test)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "# Train and evaluate LightGBM\n",
        "lgb_best_model, lgb_accuracy, lgb_feature_importance = train_and_evaluate_lightgbm(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Compare the models\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "print(f\"XGBoost Best Accuracy: {xgb_accuracy:.4f}\")\n",
        "print(f\"LightGBM Best Accuracy: {lgb_accuracy:.4f}\")\n",
        "\n",
        "# Select the best overall model\n",
        "if xgb_accuracy > lgb_accuracy:\n",
        "    best_model = xgb_best_model\n",
        "    best_model_name = \"XGBoost\"\n",
        "    best_accuracy = xgb_accuracy\n",
        "    best_feature_importance = xgb_feature_importance\n",
        "else:\n",
        "    best_model = lgb_best_model\n",
        "    best_model_name = \"LightGBM\"\n",
        "    best_accuracy = lgb_accuracy\n",
        "    best_feature_importance = lgb_feature_importance\n",
        "\n",
        "print(f\"\\nBest Overall Model: {best_model_name} with accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Save the best overall model\n",
        "overall_save_result = SaveModel(best_model, f\"Best_Overall_{best_model_name}_Model_\")\n",
        "print(overall_save_result)\n",
        "\n",
        "# Plot feature importance for the best model\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(best_feature_importance['Feature'], best_feature_importance['Importance'])\n",
        "plt.title(f'Feature Importance ({best_model_name} Model)')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'feature_importance_{best_model_name.lower()}.png')\n",
        "plt.show()\n",
        "\n",
        "# Save encoder for future use\n",
        "joblib.dump(label_encoder, f\"label_encoder_{datetime.date.today()}.pkl\")\n",
        "print(f\"Label encoder saved for future predictions\")\n",
        "\n",
        "print(\"\\nScript execution completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install xgboost lightgbm scikit-learn pandas numpy matplotlib\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Load your data\n",
        "file_name = \"Biomarker_16032025_without_SWD_with_GNV.xlsx\"\n",
        "data = pd.read_excel(file_name)\n",
        "\n",
        "# Define columns to process\n",
        "columns_to_process = ['IgG1 Average', 'IgG2 Average', 'IgG3 Average', 'IgG4 Average',\n",
        "                     'IgA Average', 'IgE Average', 'IgM Average']\n",
        "\n",
        "# Extract features and target\n",
        "base_features = data[columns_to_process]\n",
        "target = data['Group']\n",
        "\n",
        "# Apply label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_target = label_encoder.fit_transform(target)\n",
        "\n",
        "print(\"Enhanced Model Training and Evaluation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ===== 1. FEATURE ENGINEERING =====\n",
        "print(\"\\n1. Creating engineered features...\")\n",
        "\n",
        "# Create feature engineering function with better handling of edge cases\n",
        "def engineer_features(df):\n",
        "    # Create a copy of the original features\n",
        "    engineered = df.copy()\n",
        "\n",
        "    # First, replace zeros and negative values with a small positive number\n",
        "    # to avoid division by zero and log of zero/negative\n",
        "    df_safe = df.copy()\n",
        "    for col in df_safe.columns:\n",
        "        # Replace zeros and negative values with a small positive number\n",
        "        df_safe[col] = df_safe[col].replace(0, 1e-6)\n",
        "        df_safe[col] = df_safe[col].clip(lower=1e-6)  # Ensure all values are positive\n",
        "\n",
        "    # Add ratio features with safe division\n",
        "    # IgG class ratios\n",
        "    engineered['IgG1_IgG2_ratio'] = df_safe['IgG1 Average'] / df_safe['IgG2 Average']\n",
        "    engineered['IgG1_IgG3_ratio'] = df_safe['IgG1 Average'] / df_safe['IgG3 Average']\n",
        "    engineered['IgG1_IgG4_ratio'] = df_safe['IgG1 Average'] / df_safe['IgG4 Average']\n",
        "\n",
        "    # Class comparison ratios\n",
        "    engineered['IgG_IgA_ratio'] = (df_safe['IgG1 Average'] + df_safe['IgG2 Average'] +\n",
        "                               df_safe['IgG3 Average'] + df_safe['IgG4 Average']) / df_safe['IgA Average']\n",
        "    engineered['IgA_IgM_ratio'] = df_safe['IgA Average'] / df_safe['IgM Average']\n",
        "    engineered['IgE_IgM_ratio'] = df_safe['IgE Average'] / df_safe['IgM Average']\n",
        "\n",
        "    # Square terms for key features (based on feature importance)\n",
        "    engineered['IgA_squared'] = df['IgA Average'] ** 2\n",
        "    engineered['IgM_squared'] = df['IgM Average'] ** 2\n",
        "    engineered['IgE_squared'] = df['IgE Average'] ** 2\n",
        "\n",
        "    # Log transformations (using safe values)\n",
        "    for col in columns_to_process:\n",
        "        engineered[f'{col}_log'] = np.log(df_safe[col])\n",
        "\n",
        "    # Clip ratios to avoid extreme values\n",
        "    ratio_columns = [col for col in engineered.columns if 'ratio' in col]\n",
        "    for col in ratio_columns:\n",
        "        # Clip to reasonable range, e.g., between 0.01 and 100\n",
        "        engineered[col] = engineered[col].clip(lower=0.01, upper=100)\n",
        "\n",
        "    # Check for and fix any remaining infinity or NaN values\n",
        "    engineered.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    engineered.fillna(engineered.median(), inplace=True)\n",
        "\n",
        "    return engineered\n",
        "\n",
        "# Create engineered features\n",
        "engineered_features = engineer_features(base_features)\n",
        "\n",
        "print(f\"Original feature count: {base_features.shape[1]}\")\n",
        "print(f\"Engineered feature count: {engineered_features.shape[1]}\")\n",
        "\n",
        "# Check for any remaining problematic values\n",
        "print(\"\\nChecking for problematic values after engineering:\")\n",
        "print(f\"Infinity values: {np.isinf(engineered_features.values).sum()}\")\n",
        "print(f\"NaN values: {np.isnan(engineered_features.values).sum()}\")\n",
        "\n",
        "# ===== 2. OUTLIER HANDLING =====\n",
        "print(\"\\n2. Handling outliers...\")\n",
        "\n",
        "# Create robust scaled features\n",
        "robust_scaler = RobustScaler()\n",
        "robust_features = pd.DataFrame(\n",
        "    robust_scaler.fit_transform(engineered_features),\n",
        "    columns=engineered_features.columns\n",
        ")\n",
        "\n",
        "# ===== 3. ENHANCED CROSS-VALIDATION =====\n",
        "print(\"\\n3. Setting up enhanced cross-validation...\")\n",
        "\n",
        "# Use repeated stratified k-fold for more stable estimates\n",
        "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "\n",
        "# ===== 4. FIND BEST MODELS =====\n",
        "print(\"\\n4. Training and evaluating base models...\")\n",
        "\n",
        "# Define parameter grids for fine-tuning - using more focused grids based on prior results\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'min_child_weight': [3, 5],\n",
        "    'gamma': [0, 0.1],\n",
        "    'reg_alpha': [0.1, 0.5],\n",
        "    'reg_lambda': [0.1, 1.0]\n",
        "}\n",
        "\n",
        "lgb_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [5, 6, 7],\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'num_leaves': [31, 50],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'min_child_samples': [5, 10],\n",
        "    'reg_alpha': [0.1, 0.5],\n",
        "    'reg_lambda': [0.1, 1.0]\n",
        "}\n",
        "\n",
        "# Create smaller grids for faster results\n",
        "xgb_quick_param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [5],\n",
        "    'learning_rate': [0.1],\n",
        "    'subsample': [0.9],\n",
        "    'colsample_bytree': [0.9],\n",
        "    'min_child_weight': [3],\n",
        "    'gamma': [0.1],\n",
        "    'reg_alpha': [0.5],\n",
        "    'reg_lambda': [1.0]\n",
        "}\n",
        "\n",
        "lgb_quick_param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [5],\n",
        "    'learning_rate': [0.1],\n",
        "    'num_leaves': [31],\n",
        "    'subsample': [0.9],\n",
        "    'colsample_bytree': [0.9],\n",
        "    'min_child_samples': [10],\n",
        "    'reg_alpha': [0.5],\n",
        "    'reg_lambda': [1.0]\n",
        "}\n",
        "\n",
        "# Choose whether to use quick grids (faster) or full grids (better results)\n",
        "use_quick_grid = True  # Set to False for more thorough but slower search\n",
        "\n",
        "xgb_grid = xgb_quick_param_grid if use_quick_grid else xgb_param_grid\n",
        "lgb_grid = lgb_quick_param_grid if use_quick_grid else lgb_param_grid\n",
        "\n",
        "# Function to find best model\n",
        "def find_best_model(model_class, param_grid, features, target, cv, name):\n",
        "    print(f\"\\nFinding best {name} model...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        model_class(),\n",
        "        param_grid=param_grid,\n",
        "        cv=cv,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(features, target)\n",
        "\n",
        "    print(f\"Best {name} parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best {name} CV accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
        "\n",
        "# Find best models using robust scaled engineered features\n",
        "xgb_model, xgb_best_params, xgb_cv_score = find_best_model(\n",
        "    xgb.XGBClassifier,\n",
        "    xgb_grid,\n",
        "    robust_features,\n",
        "    encoded_target,\n",
        "    cv,\n",
        "    \"XGBoost\"\n",
        ")\n",
        "\n",
        "lgb_model, lgb_best_params, lgb_cv_score = find_best_model(\n",
        "    lgb.LGBMClassifier,\n",
        "    lgb_grid,\n",
        "    robust_features,\n",
        "    encoded_target,\n",
        "    cv,\n",
        "    \"LightGBM\"\n",
        ")\n",
        "\n",
        "# ===== 5. CREATE ENSEMBLE MODEL =====\n",
        "print(\"\\n5. Creating ensemble model...\")\n",
        "\n",
        "# Create a voting classifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb.XGBClassifier(**xgb_best_params)),\n",
        "        ('lgb', lgb.LGBMClassifier(**lgb_best_params))\n",
        "    ],\n",
        "    voting='soft'  # Use predicted probabilities\n",
        ")\n",
        "\n",
        "# Evaluate the voting classifier\n",
        "cv_scores = cross_val_score(voting_clf, robust_features, encoded_target, cv=cv, scoring='accuracy')\n",
        "print(f\"Voting Classifier CV scores: {cv_scores}\")\n",
        "print(f\"Voting Classifier mean CV accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "# ===== 6. FINAL EVALUATION =====\n",
        "print(\"\\n6. Final evaluation with hold-out test set...\")\n",
        "\n",
        "# Create a train/validation/test split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    robust_features, encoded_target, test_size=0.3, random_state=42, stratify=encoded_target\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Train the ensemble model\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on validation set\n",
        "val_pred = voting_clf.predict(X_val)\n",
        "val_acc = accuracy_score(y_val, val_pred)\n",
        "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_pred = voting_clf.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "print(classification_report(y_test, test_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix (Test Set):\")\n",
        "conf_matrix = confusion_matrix(y_test, test_pred)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Convert to original class names\n",
        "y_test_names = label_encoder.inverse_transform(y_test)\n",
        "pred_test_names = label_encoder.inverse_transform(test_pred)\n",
        "\n",
        "print(\"\\nConfusion Matrix with class names:\")\n",
        "conf_df = pd.DataFrame(\n",
        "    confusion_matrix(y_test_names, pred_test_names),\n",
        "    index=label_encoder.classes_,\n",
        "    columns=label_encoder.classes_\n",
        ")\n",
        "print(conf_df)\n",
        "\n",
        "# ===== 7. FEATURE IMPORTANCE =====\n",
        "print(\"\\n7. Feature importance...\")\n",
        "\n",
        "# Get XGBoost feature importance\n",
        "xgb_importance = pd.DataFrame({\n",
        "    'Feature': robust_features.columns,\n",
        "    'XGBoost Importance': xgb_model.feature_importances_\n",
        "}).sort_values('XGBoost Importance', ascending=False)\n",
        "\n",
        "# Get LightGBM feature importance\n",
        "lgb_importance = pd.DataFrame({\n",
        "    'Feature': robust_features.columns,\n",
        "    'LightGBM Importance': lgb_model.feature_importances_\n",
        "}).sort_values('LightGBM Importance', ascending=False)\n",
        "\n",
        "# Print top 10 features for each model\n",
        "print(\"\\nTop 10 XGBoost features:\")\n",
        "print(xgb_importance.head(10))\n",
        "\n",
        "print(\"\\nTop 10 LightGBM features:\")\n",
        "print(lgb_importance.head(10))\n",
        "\n",
        "# ===== 8. SAVE BEST MODEL =====\n",
        "print(\"\\n8. Saving best model...\")\n",
        "\n",
        "# Save the ensemble model\n",
        "best_model_filename = f\"Best_Ensemble_Model_{datetime.date.today()}.pkl\"\n",
        "joblib.dump(voting_clf, best_model_filename)\n",
        "print(f\"Ensemble model saved to '{best_model_filename}'\")\n",
        "\n",
        "# Save feature engineering pipeline for future use\n",
        "robust_scaler_filename = f\"robust_scaler_{datetime.date.today()}.pkl\"\n",
        "joblib.dump(robust_scaler, robust_scaler_filename)\n",
        "print(f\"Robust scaler saved to '{robust_scaler_filename}'\")\n",
        "\n",
        "# Save label encoder\n",
        "label_encoder_filename = f\"label_encoder_{datetime.date.today()}.pkl\"\n",
        "joblib.dump(label_encoder, label_encoder_filename)\n",
        "print(f\"Label encoder saved to '{label_encoder_filename}'\")\n",
        "\n",
        "print(\"\\nEnhanced modeling complete!\")\n",
        "\n",
        "# Final result summary\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"XGBoost CV Score: {xgb_cv_score:.4f}\")\n",
        "print(f\"LightGBM CV Score: {lgb_cv_score:.4f}\")\n",
        "print(f\"Ensemble CV Score: {cv_scores.mean():.4f}\")\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "9kCcs9X_CY-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import random\n",
        "\n",
        "# Load your data\n",
        "file_name = \"Biomarker_16032025_without_SWD_with_GNV.xlsx\"\n",
        "data = pd.read_excel(file_name)\n",
        "\n",
        "# Define columns to process\n",
        "columns_to_process = ['IgG1 Average', 'IgG2 Average', 'IgG3 Average', 'IgG4 Average',\n",
        "                      'IgA Average', 'IgE Average', 'IgM Average']\n",
        "\n",
        "# Extract features and target\n",
        "features = data[columns_to_process]\n",
        "target = data['Group']\n",
        "\n",
        "# Load your previously saved models\n",
        "xgb_model = joblib.load(\"XGBoost_Best_Model_2025-03-20.pkl\")\n",
        "lgb_model = joblib.load(\"LightGBM_Best_Model_2025-03-20.pkl\")\n",
        "label_encoder = joblib.load(\"label_encoder_2025-03-20.pkl\")\n",
        "\n",
        "# Encode target if needed (may be unnecessary if already done)\n",
        "encoded_target = label_encoder.transform(target)\n",
        "\n",
        "print(\"Model Validation Tests\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Multiple Random State Validation\n",
        "print(\"\\n1. Testing with multiple random states:\")\n",
        "for seed in [42, 100, 200, 300, 400]:\n",
        "    # Split with different random states\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, encoded_target, test_size=0.3,\n",
        "        random_state=seed, stratify=encoded_target\n",
        "    )\n",
        "\n",
        "    # Standardize features (important: fit only on training data)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)  # Only transform test data\n",
        "\n",
        "    # Make predictions with both models\n",
        "    xgb_pred = xgb_model.predict(X_test_scaled)\n",
        "    lgb_pred = lgb_model.predict(X_test_scaled)\n",
        "\n",
        "    # Calculate accuracies\n",
        "    xgb_acc = accuracy_score(y_test, xgb_pred)\n",
        "    lgb_acc = accuracy_score(y_test, lgb_pred)\n",
        "\n",
        "    print(f\"Random state {seed}:\")\n",
        "    print(f\"  XGBoost accuracy: {xgb_acc:.4f}\")\n",
        "    print(f\"  LightGBM accuracy: {lgb_acc:.4f}\")\n",
        "\n",
        "# 2. K-fold Cross-validation\n",
        "print(\"\\n2. K-fold cross-validation:\")\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "xgb_cv_scores = []\n",
        "lgb_cv_scores = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(features):\n",
        "    # Split data\n",
        "    X_train, X_test = features.iloc[train_idx], features.iloc[test_idx]\n",
        "    y_train, y_test = encoded_target[train_idx], encoded_target[test_idx]\n",
        "\n",
        "    # Standardize\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Train fresh models with same hyperparameters\n",
        "    # For XGBoost\n",
        "    xgb_params = xgb_model.get_params()\n",
        "    xgb_fresh = xgb.XGBClassifier(**xgb_params)\n",
        "    xgb_fresh.fit(X_train_scaled, y_train)\n",
        "    xgb_pred = xgb_fresh.predict(X_test_scaled)\n",
        "    xgb_cv_scores.append(accuracy_score(y_test, xgb_pred))\n",
        "\n",
        "    # For LightGBM\n",
        "    lgb_params = lgb_model.get_params()\n",
        "    lgb_fresh = lgb.LGBMClassifier(**lgb_params)\n",
        "    lgb_fresh.fit(X_train_scaled, y_train)\n",
        "    lgb_pred = lgb_fresh.predict(X_test_scaled)\n",
        "    lgb_cv_scores.append(accuracy_score(y_test, lgb_pred))\n",
        "\n",
        "print(f\"XGBoost CV scores: {xgb_cv_scores}\")\n",
        "print(f\"XGBoost mean CV accuracy: {np.mean(xgb_cv_scores):.4f}\")\n",
        "print(f\"LightGBM CV scores: {lgb_cv_scores}\")\n",
        "print(f\"LightGBM mean CV accuracy: {np.mean(lgb_cv_scores):.4f}\")\n",
        "\n",
        "# 3. Feature permutation test\n",
        "print(\"\\n3. Feature permutation test:\")\n",
        "# Create a baseline dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, encoded_target, test_size=0.3, random_state=42, stratify=encoded_target\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Get baseline accuracy\n",
        "xgb_baseline = xgb_model.predict(X_test_scaled)\n",
        "lgb_baseline = lgb_model.predict(X_test_scaled)\n",
        "xgb_baseline_acc = accuracy_score(y_test, xgb_baseline)\n",
        "lgb_baseline_acc = accuracy_score(y_test, lgb_baseline)\n",
        "\n",
        "# Test importance of each feature by permutation\n",
        "for col_idx, column in enumerate(columns_to_process):\n",
        "    # Create a copy of the test data\n",
        "    X_test_permuted = X_test_scaled.copy()\n",
        "\n",
        "    # Shuffle the column values\n",
        "    np.random.seed(42)\n",
        "    X_test_permuted[:,col_idx] = np.random.permutation(X_test_permuted[:,col_idx])\n",
        "\n",
        "    # Make predictions with permuted data\n",
        "    xgb_perm_pred = xgb_model.predict(X_test_permuted)\n",
        "    lgb_perm_pred = lgb_model.predict(X_test_permuted)\n",
        "\n",
        "    # Calculate new accuracy\n",
        "    xgb_perm_acc = accuracy_score(y_test, xgb_perm_pred)\n",
        "    lgb_perm_acc = accuracy_score(y_test, lgb_perm_pred)\n",
        "\n",
        "    # Calculate importance as drop in accuracy\n",
        "    xgb_importance = xgb_baseline_acc - xgb_perm_acc\n",
        "    lgb_importance = lgb_baseline_acc - lgb_perm_acc\n",
        "\n",
        "    print(f\"Feature: {column}\")\n",
        "    print(f\"  XGBoost importance: {xgb_importance:.4f}\")\n",
        "    print(f\"  LightGBM importance: {lgb_importance:.4f}\")\n",
        "\n",
        "# 4. Three-way split validation\n",
        "print(\"\\n4. Three-way split validation:\")\n",
        "# Split into train, validation, test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    features, encoded_target, test_size=0.4, random_state=42, stratify=encoded_target\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train new models with same hyperparameters\n",
        "xgb_params = xgb_model.get_params()\n",
        "lgb_params = lgb_model.get_params()\n",
        "\n",
        "new_xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "new_lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "new_xgb_model.fit(X_train_scaled, y_train)\n",
        "new_lgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate on validation set\n",
        "xgb_val_pred = new_xgb_model.predict(X_val_scaled)\n",
        "lgb_val_pred = new_lgb_model.predict(X_val_scaled)\n",
        "\n",
        "xgb_val_acc = accuracy_score(y_val, xgb_val_pred)\n",
        "lgb_val_acc = accuracy_score(y_val, lgb_val_pred)\n",
        "\n",
        "print(\"Validation set results:\")\n",
        "print(f\"  XGBoost accuracy: {xgb_val_acc:.4f}\")\n",
        "print(f\"  LightGBM accuracy: {lgb_val_acc:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "xgb_test_pred = new_xgb_model.predict(X_test_scaled)\n",
        "lgb_test_pred = new_lgb_model.predict(X_test_scaled)\n",
        "\n",
        "xgb_test_acc = accuracy_score(y_test, xgb_test_pred)\n",
        "lgb_test_acc = accuracy_score(y_test, lgb_test_pred)\n",
        "\n",
        "print(\"Test set results:\")\n",
        "print(f\"  XGBoost accuracy: {xgb_test_acc:.4f}\")\n",
        "print(f\"  LightGBM accuracy: {lgb_test_acc:.4f}\")\n",
        "\n",
        "# 5. Learning curve\n",
        "print(\"\\n5. Learning curve analysis:\")\n",
        "train_sizes = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
        "xgb_train_scores = []\n",
        "xgb_test_scores = []\n",
        "lgb_train_scores = []\n",
        "lgb_test_scores = []\n",
        "\n",
        "# Create a fixed test set\n",
        "X_full_train, X_fixed_test, y_full_train, y_fixed_test = train_test_split(\n",
        "    features, encoded_target, test_size=0.2, random_state=42, stratify=encoded_target\n",
        ")\n",
        "scaler = StandardScaler()\n",
        "X_full_train_scaled = scaler.fit_transform(X_full_train)\n",
        "X_fixed_test_scaled = scaler.transform(X_fixed_test)\n",
        "\n",
        "for size in train_sizes:\n",
        "    # Take a subset of training data\n",
        "    subset_size = int(len(X_full_train) * size)\n",
        "    indices = random.sample(range(len(X_full_train)), subset_size)\n",
        "\n",
        "    X_subset = X_full_train_scaled[indices]\n",
        "    y_subset = y_full_train.iloc[indices] if hasattr(y_full_train, 'iloc') else y_full_train[indices]\n",
        "\n",
        "    # Train models\n",
        "    subset_xgb = xgb.XGBClassifier(**xgb_params)\n",
        "    subset_lgb = lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "    subset_xgb.fit(X_subset, y_subset)\n",
        "    subset_lgb.fit(X_subset, y_subset)\n",
        "\n",
        "    # Evaluate on training set\n",
        "    xgb_train_pred = subset_xgb.predict(X_subset)\n",
        "    lgb_train_pred = subset_lgb.predict(X_subset)\n",
        "\n",
        "    xgb_train_acc = accuracy_score(y_subset, xgb_train_pred)\n",
        "    lgb_train_acc = accuracy_score(y_subset, lgb_train_pred)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    xgb_test_pred = subset_xgb.predict(X_fixed_test_scaled)\n",
        "    lgb_test_pred = subset_lgb.predict(X_fixed_test_scaled)\n",
        "\n",
        "    xgb_test_acc = accuracy_score(y_fixed_test, xgb_test_pred)\n",
        "    lgb_test_acc = accuracy_score(y_fixed_test, lgb_test_pred)\n",
        "\n",
        "    # Store results\n",
        "    xgb_train_scores.append(xgb_train_acc)\n",
        "    xgb_test_scores.append(xgb_test_acc)\n",
        "    lgb_train_scores.append(lgb_train_acc)\n",
        "    lgb_test_scores.append(lgb_test_acc)\n",
        "\n",
        "    print(f\"Training size: {size*100:.0f}%\")\n",
        "    print(f\"  XGBoost - Train: {xgb_train_acc:.4f}, Test: {xgb_test_acc:.4f}\")\n",
        "    print(f\"  LightGBM - Train: {lgb_train_acc:.4f}, Test: {lgb_test_acc:.4f}\")\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_sizes, xgb_train_scores, 'o-', label='Training accuracy')\n",
        "plt.plot(train_sizes, xgb_test_scores, 'o-', label='Test accuracy')\n",
        "plt.title('XGBoost Learning Curve')\n",
        "plt.xlabel('Training Set Size (proportion)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_sizes, lgb_train_scores, 'o-', label='Training accuracy')\n",
        "plt.plot(train_sizes, lgb_test_scores, 'o-', label='Test accuracy')\n",
        "plt.title('LightGBM Learning Curve')\n",
        "plt.xlabel('Training Set Size (proportion)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('learning_curves.png')\n",
        "\n",
        "print(\"\\nValidation complete. Learning curves saved to 'learning_curves.png'\")"
      ],
      "metadata": {
        "id": "lG6pPDCTCbMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}